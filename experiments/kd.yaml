# Knowledge Distillation Configuration
# This configuration trains LightNet with knowledge distillation from a teacher model

# Model configuration
model:
  input_shape: [224, 224, 3]
  num_classes: 4
  version: "v1"  # "v1" or "v2"
  dropout_rate: 0.3
  use_se: true
  channel_multiplier: 1.0

# Data configuration
data:
  batch_size: 32
  image_size: [224, 224]
  augmentation:
    horizontal_flip: true
    vertical_flip: false
    rotation_range: 15.0
    zoom_range: 0.1
    brightness_range: 0.1
    contrast_range: 0.1
    noise_std: 0.01

# Training configuration
training:
  epochs: 100
  optimizer:
    type: "adamw"
    learning_rate: 0.001
    weight_decay: 0.01
  loss: "categorical_crossentropy"
  label_smoothing: 0.0
  use_class_weights: true
  monitor: "val_accuracy"
  monitor_mode: "max"
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 15
    
  # Learning rate scheduler
  lr_scheduler:
    enabled: true
    type: "cosine"
    first_decay_steps: 50
    t_mul: 2.0
    m_mul: 1.0
    alpha: 0.0
    
  # TensorBoard
  tensorboard:
    enabled: true

# Knowledge distillation configuration
distillation:
  # Soft target distillation
  temperature: 3.0
  alpha: 0.5  # Weight for soft target loss
  
  # Feature distillation
  beta: 0.3   # Weight for feature distillation loss
  gamma: 0.2  # Weight for hard target loss
  
  # Feature layers for distillation
  feature_layers:
    - "lite_dr2"
    - "lite_dr3"
    - "lite_dr4"
  
  # Feature layer weights
  feature_weights: [0.3, 0.4, 0.3]

# Evaluation configuration
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "cohen_kappa"
    - "roc_auc"
  
  # GradCAM visualization
  gradcam:
    enabled: true
    num_samples_per_class: 2
    layer_name: null  # Auto-detect last conv layer
  
  # Model comparison
  compare_with_teacher: true
